{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "We use automatic differentiation to compute the gradient automatically, saving us the trouble of writing the calculations by hand.\n",
    "\n",
    "### Elves' execution (everything by hand)\n",
    "![no graph](./images/no_graph.png)\n",
    "\n",
    "Elves have a lot of time to waste, since they are basically immortal. If you are too, you are allowed to ignore automatic differentiation...\n",
    "\n",
    "### The static computational graph (deferred execution)\n",
    "![static graph](./images/static_graph.png)\n",
    "\n",
    "The neuron gets compiled into a symbolic graph in which each node represents individual operations (second row), using placeholders for inputs and outputs\n",
    "\n",
    "### The dynamic computational graph (immediate execution)\n",
    "![dynamic graph](./images/dynamic_graph.png)\n",
    "\n",
    "The computational graph is built node by node as the code is eagerly evaluated. It easier to accomplish conditional behavior, since the CG can change during successive forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1., 2], [3, 4]], requires_grad=True)\n",
    "b = torch.ones((2, 2), requires_grad=True)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "d = b * c\n",
    "d = d + a\n",
    "d.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "e = torch.mean(d) + torch.mean(b)\n",
    "make_dot(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()\n",
    "# for memory efficiency, the graph is deleted during the backward\n",
    "# e.backward() # error\n",
    "\n",
    "# if we want not the graph to be freed, specify retain_graph=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad # it is a leaf of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.grad # it is not a leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.grad # it is not a leaf, but has the attribute retains_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic graph - (define-by-run)\n",
    "# it reminds of the difference between Python variables and C variables\n",
    "x = torch.tensor([1., 2.], requires_grad=True)\n",
    "y = x.sum()\n",
    "\n",
    "while y.data.norm() < 12:\n",
    "    y = y * 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you cannot get a numpy array from a tensor which requires grad\n",
    "# x.numpy() # error\n",
    "\n",
    "# we must detach it from the computational graph\n",
    "print(x)\n",
    "print(x.detach())\n",
    "print(x.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both detach() and numpy() are both views on the same storage\n",
    "x_dn = x.detach().numpy()\n",
    "x[0] = 99\n",
    "print(x_dn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "[Automatic differentiation in Pytorch](https://openreview.net/pdf?id=BJJsrmfCZ)\n",
    "\n",
    "[Autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "\n",
    "[Nice overview on Pytorch (the pictures above are taken from here!)](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
